import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import io
import json
from data_pipeline import MenariniDataPipeline
from visualization import DataVisualization

# Configure page
st.set_page_config(
    page_title="Menarini Asia Pacific - Data Pipeline",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for Chinese font support
st.markdown("""
<style>
    .stApp {
        font-family: "SimHei", "Arial Unicode MS", "DejaVu Sans", sans-serif;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .success-message {
        background-color: #d4edda;
        color: #155724;
        padding: 0.75rem;
        border-radius: 0.25rem;
        border: 1px solid #c3e6cb;
    }
    .error-message {
        background-color: #f8d7da;
        color: #721c24;
        padding: 0.75rem;
        border-radius: 0.25rem;
        border: 1px solid #f5c6cb;
    }
</style>
""", unsafe_allow_html=True)

def initialize_session_state():
    """Initialize session state variables"""
    if 'pipeline' not in st.session_state:
        st.session_state.pipeline = None
    if 'data_processed' not in st.session_state:
        st.session_state.data_processed = False
    if 'uploaded_file' not in st.session_state:
        st.session_state.uploaded_file = None

def display_header():
    """Display application header"""
    st.title("ğŸ¥ Menarini Asia Pacific - æ•°æ®ç®¡é“ç³»ç»Ÿ")
    st.markdown("---")
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("æ•°æ®æº", "Excelæ–‡ä»¶", "æ”¯æŒå¤šå·¥ä½œè¡¨")
    with col2:
        st.metric("å¸‚åœºç»†åˆ†", "6ä¸ª", "RX, ç”µå•†, è®¾å¤‡ç­‰")
    with col3:
        st.metric("æ•°æ®æ²»ç†", "è‡ªåŠ¨åŒ–", "è´¨é‡æ£€æŸ¥")
    with col4:
        st.metric("çŠ¶æ€", "å°±ç»ª" if st.session_state.data_processed else "ç­‰å¾…æ•°æ®", "")

def file_upload_section():
    """Handle file upload and processing"""
    st.header("ğŸ“ æ•°æ®ä¸Šä¼ ä¸å¤„ç†")
    
    uploaded_file = st.file_uploader(
        "ä¸Šä¼  Excel æ–‡ä»¶ (Case Study - Data & AI Engineer.xlsx)",
        type=['xlsx', 'xls'],
        help="è¯·ä¸Šä¼ åŒ…å«å¸‚åœºæ•°æ®çš„Excelæ–‡ä»¶"
    )
    
    if uploaded_file is not None:
        if uploaded_file != st.session_state.uploaded_file:
            st.session_state.uploaded_file = uploaded_file
            st.session_state.data_processed = False
            st.session_state.pipeline = None
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.info(f"ğŸ“„ æ–‡ä»¶å·²ä¸Šä¼ : {uploaded_file.name}")
            st.write(f"æ–‡ä»¶å¤§å°: {uploaded_file.size / 1024 / 1024:.2f} MB")
        
        with col2:
            if st.button("ğŸš€ å¼€å§‹å¤„ç†æ•°æ®", type="primary"):
                process_data(uploaded_file)
    
    return uploaded_file

def process_data(uploaded_file):
    """Process uploaded data through the pipeline"""
    with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®ï¼Œè¯·ç¨å€™..."):
        try:
            # Save uploaded file temporarily
            file_bytes = uploaded_file.read()
            
            # Initialize pipeline
            pipeline = MenariniDataPipeline()
            
            # Extract data from uploaded file
            with st.expander("ğŸ“Š æ•°æ®æå–æ—¥å¿—", expanded=False):
                log_container = st.empty()
                
                # Extract data
                success = pipeline.extract_data_from_bytes(file_bytes, uploaded_file.name)
                
                if success:
                    log_container.success("âœ… æ•°æ®æå–æˆåŠŸ")
                    
                    # Validate data sources
                    validation_results = pipeline.validate_data_sources()
                    
                    # Perform ETL process
                    pipeline.comprehensive_etl_process()
                    
                    # Create unified data model
                    pipeline.create_unified_data_model()
                    
                    # Generate quality report
                    pipeline.assess_data_quality()
                    
                    st.session_state.pipeline = pipeline
                    st.session_state.data_processed = True
                    
                    st.success("ğŸ‰ æ•°æ®å¤„ç†å®Œæˆï¼")
                    st.rerun()
                else:
                    log_container.error("âŒ æ•°æ®æå–å¤±è´¥")
                    
        except Exception as e:
            st.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")

def display_data_overview():
    """Display data overview and quality metrics"""
    if not st.session_state.data_processed or not st.session_state.pipeline:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ•°æ®æ–‡ä»¶")
        return
    
    st.header("ğŸ“ˆ æ•°æ®æ¦‚è§ˆ")
    
    pipeline = st.session_state.pipeline
    
    # Display bronze layer info
    st.subheader("ğŸ¥‰ åŸå§‹æ•°æ®å±‚ (Bronze Layer)")
    bronze_data = pipeline.data_lake['bronze']
    
    cols = st.columns(len(bronze_data))
    for i, (sheet_name, sheet_info) in enumerate(bronze_data.items()):
        with cols[i % len(cols)]:
            st.metric(
                label=sheet_name,
                value=f"{sheet_info['original_shape'][0]} è¡Œ",
                delta=f"{sheet_info['original_shape'][1]} åˆ—"
            )
    
    # Display silver layer info
    st.subheader("ğŸ¥ˆ æ¸…æ´—æ•°æ®å±‚ (Silver Layer)")
    silver_data = pipeline.data_lake['silver']
    
    if silver_data:
        df_info = []
        for sheet_name, sheet_info in silver_data.items():
            df_info.append({
                'å·¥ä½œè¡¨': sheet_name,
                'è¡Œæ•°': sheet_info['final_shape'][0],
                'åˆ—æ•°': sheet_info['final_shape'][1],
                'è½¬æ¢æ­¥éª¤': sheet_info['transformations_applied'],
                'å¤„ç†æ—¶é—´': sheet_info['processed_at'].strftime('%Y-%m-%d %H:%M:%S')
            })
        
        st.dataframe(pd.DataFrame(df_info), use_container_width=True)
    
    # Display gold layer info
    st.subheader("ğŸ¥‡ ä¸šåŠ¡æ•°æ®å±‚ (Gold Layer)")
    if 'unified_model' in pipeline.data_lake['gold']:
        unified_data = pipeline.data_lake['gold']['unified_model']['data']
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ç»Ÿä¸€æ•°æ®é›†è¡Œæ•°", f"{len(unified_data):,}")
        with col2:
            st.metric("ç»Ÿä¸€æ•°æ®é›†åˆ—æ•°", len(unified_data.columns))
        with col3:
            total_qty = unified_data[[col for col in unified_data.columns if 'QTY' in col or 'æ•°é‡' in col]].sum().sum()
            st.metric("æ€»é”€å”®æ•°é‡", f"{total_qty:,.0f}")

def display_data_quality():
    """Display data quality assessment"""
    if not st.session_state.data_processed or not st.session_state.pipeline:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ•°æ®æ–‡ä»¶")
        return
    
    st.header("ğŸ” æ•°æ®è´¨é‡è¯„ä¼°")
    
    pipeline = st.session_state.pipeline
    
    if hasattr(pipeline, 'data_quality_report') and pipeline.data_quality_report:
        # Overall quality metrics
        st.subheader("æ•´ä½“è´¨é‡æŒ‡æ ‡")
        
        total_completeness = 0
        total_validity = 0
        total_consistency = 0
        sheet_count = 0
        
        quality_data = []
        
        for sheet_name, quality_metrics in pipeline.data_quality_report.items():
            if isinstance(quality_metrics, dict):
                completeness = quality_metrics.get('completeness', 0)
                validity = quality_metrics.get('validity', 0)
                consistency = quality_metrics.get('consistency', 0)
                
                total_completeness += completeness
                total_validity += validity
                total_consistency += consistency
                sheet_count += 1
                
                quality_data.append({
                    'å·¥ä½œè¡¨': sheet_name,
                    'å®Œæ•´æ€§': f"{completeness:.1%}",
                    'æœ‰æ•ˆæ€§': f"{validity:.1%}",
                    'ä¸€è‡´æ€§': f"{consistency:.1%}",
                    'æ•´ä½“è¯„åˆ†': f"{(completeness + validity + consistency) / 3:.1%}"
                })
        
        if sheet_count > 0:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                avg_completeness = total_completeness / sheet_count
                st.metric("å¹³å‡å®Œæ•´æ€§", f"{avg_completeness:.1%}")
            
            with col2:
                avg_validity = total_validity / sheet_count
                st.metric("å¹³å‡æœ‰æ•ˆæ€§", f"{avg_validity:.1%}")
            
            with col3:
                avg_consistency = total_consistency / sheet_count
                st.metric("å¹³å‡ä¸€è‡´æ€§", f"{avg_consistency:.1%}")
            
            # Quality details table
            st.subheader("è¯¦ç»†è´¨é‡æŠ¥å‘Š")
            st.dataframe(pd.DataFrame(quality_data), use_container_width=True)
    else:
        st.info("æ•°æ®è´¨é‡æŠ¥å‘Šæ­£åœ¨ç”Ÿæˆä¸­...")

def display_market_analysis():
    """Display market segment analysis"""
    if not st.session_state.data_processed or not st.session_state.pipeline:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ•°æ®æ–‡ä»¶")
        return
    
    st.header("ğŸª å¸‚åœºç»†åˆ†åˆ†æ")
    
    pipeline = st.session_state.pipeline
    
    if 'unified_model' in pipeline.data_lake['gold']:
        unified_data = pipeline.data_lake['gold']['unified_model']['data']
        
        # Market type distribution
        if 'å¸‚åœºç±»å‹' in unified_data.columns:
            st.subheader("å¸‚åœºç±»å‹åˆ†å¸ƒ")
            
            market_dist = unified_data['å¸‚åœºç±»å‹'].value_counts()
            
            col1, col2 = st.columns([1, 1])
            
            with col1:
                fig_pie = px.pie(
                    values=market_dist.values,
                    names=market_dist.index,
                    title="å¸‚åœºç±»å‹åˆ†å¸ƒ"
                )
                st.plotly_chart(fig_pie, use_container_width=True)
            
            with col2:
                st.dataframe(
                    pd.DataFrame({
                        'å¸‚åœºç±»å‹': market_dist.index,
                        'è®°å½•æ•°': market_dist.values,
                        'å æ¯”': (market_dist.values / market_dist.sum() * 100).round(1)
                    }),
                    use_container_width=True
                )
        
        # Sales quantity analysis by market
        qty_cols = [col for col in unified_data.columns if 'QTY' in col or 'æ•°é‡' in col]
        if qty_cols and 'å¸‚åœºç±»å‹' in unified_data.columns:
            st.subheader("å„å¸‚åœºé”€å”®æ•°é‡åˆ†æ")
            
            qty_col = qty_cols[0]
            market_sales = unified_data.groupby('å¸‚åœºç±»å‹')[qty_col].sum().sort_values(ascending=False)
            
            fig_bar = px.bar(
                x=market_sales.values,
                y=market_sales.index,
                orientation='h',
                title="å„å¸‚åœºé”€å”®æ•°é‡",
                labels={'x': 'é”€å”®æ•°é‡', 'y': 'å¸‚åœºç±»å‹'}
            )
            st.plotly_chart(fig_bar, use_container_width=True)
        
        # Time series analysis
        date_cols = [col for col in unified_data.columns if any(k in col.lower() for k in ['date', 'æ—¥æœŸ', 'orderdate'])]
        if date_cols and qty_cols:
            st.subheader("é”€å”®è¶‹åŠ¿åˆ†æ")
            
            date_col = date_cols[0]
            qty_col = qty_cols[0]
            
            # Filter valid dates
            valid_dates = unified_data[unified_data[date_col].notna()]
            
            if not valid_dates.empty:
                time_sales = valid_dates.groupby([valid_dates[date_col].dt.to_period('M'), 'å¸‚åœºç±»å‹'])[qty_col].sum().reset_index()
                time_sales[date_col] = time_sales[date_col].astype(str)
                
                fig_line = px.line(
                    time_sales,
                    x=date_col,
                    y=qty_col,
                    color='å¸‚åœºç±»å‹',
                    title="æœˆåº¦é”€å”®è¶‹åŠ¿"
                )
                st.plotly_chart(fig_line, use_container_width=True)

def display_data_lineage():
    """Display data lineage and governance information"""
    if not st.session_state.data_processed or not st.session_state.pipeline:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ•°æ®æ–‡ä»¶")
        return
    
    st.header("ğŸ“‹ æ•°æ®è¡€ç¼˜ä¸æ²»ç†")
    
    pipeline = st.session_state.pipeline
    
    # Data lineage information
    st.subheader("æ•°æ®è¡€ç¼˜è¿½è¸ª")
    
    for sheet_name, lineage_info in pipeline.lineage_tracker.items():
        with st.expander(f"ğŸ“Š {sheet_name} æ•°æ®è¡€ç¼˜", expanded=False):
            st.write(f"**æ•°æ®æº:** {lineage_info['source']}")
            st.write(f"**æå–æ—¶é—´:** {lineage_info['extraction_timestamp']}")
            
            if lineage_info['transformations']:
                st.write("**è½¬æ¢æ­¥éª¤:**")
                for i, transform in enumerate(lineage_info['transformations'], 1):
                    st.write(f"{i}. {transform['step']} - {transform['timestamp']}")
                    if 'changes' in transform:
                        st.json(transform['changes'])
                    if 'rules_applied' in transform:
                        for rule in transform['rules_applied']:
                            st.write(f"   - {rule}")

def export_data_section():
    """Handle data export functionality"""
    if not st.session_state.data_processed or not st.session_state.pipeline:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ•°æ®æ–‡ä»¶")
        return
    
    st.header("ğŸ“¤ æ•°æ®å¯¼å‡º")
    
    pipeline = st.session_state.pipeline
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("å¯¼å‡ºç»Ÿä¸€æ•°æ®é›†")
        if 'unified_model' in pipeline.data_lake['gold']:
            unified_data = pipeline.data_lake['gold']['unified_model']['data']
            
            # Export to CSV
            csv_buffer = io.StringIO()
            unified_data.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
            
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½ CSV æ–‡ä»¶",
                data=csv_buffer.getvalue(),
                file_name=f"menarini_unified_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
            
            # Export to Excel
            excel_buffer = io.BytesIO()
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                unified_data.to_excel(writer, sheet_name='ç»Ÿä¸€æ•°æ®é›†', index=False)
                
                # Add summary sheet
                summary_data = {
                    'æŒ‡æ ‡': ['æ€»è®°å½•æ•°', 'æ€»åˆ—æ•°', 'å¤„ç†æ—¶é—´'],
                    'å€¼': [len(unified_data), len(unified_data.columns), datetime.now().strftime('%Y-%m-%d %H:%M:%S')]
                }
                pd.DataFrame(summary_data).to_excel(writer, sheet_name='æ•°æ®æ‘˜è¦', index=False)
            
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½ Excel æ–‡ä»¶",
                data=excel_buffer.getvalue(),
                file_name=f"menarini_unified_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
    
    with col2:
        st.subheader("å¯¼å‡ºè´¨é‡æŠ¥å‘Š")
        if hasattr(pipeline, 'data_quality_report') and pipeline.data_quality_report:
            # Export quality report as JSON
            quality_json = json.dumps(pipeline.data_quality_report, indent=2, ensure_ascii=False, default=str)
            
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½è´¨é‡æŠ¥å‘Š (JSON)",
                data=quality_json,
                file_name=f"data_quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )

def main():
    """Main application function"""
    initialize_session_state()
    display_header()
    
    # Sidebar navigation
    st.sidebar.title("ğŸ§­ å¯¼èˆªèœå•")
    
    menu_options = [
        "ğŸ“ æ•°æ®ä¸Šä¼ ",
        "ğŸ“ˆ æ•°æ®æ¦‚è§ˆ",
        "ğŸ” è´¨é‡è¯„ä¼°",
        "ğŸª å¸‚åœºåˆ†æ",
        "ğŸ“‹ æ•°æ®æ²»ç†",
        "ğŸ“¤ æ•°æ®å¯¼å‡º"
    ]
    
    selected_menu = st.sidebar.radio("é€‰æ‹©åŠŸèƒ½:", menu_options)
    
    # Main content area
    if selected_menu == "ğŸ“ æ•°æ®ä¸Šä¼ ":
        file_upload_section()
    elif selected_menu == "ğŸ“ˆ æ•°æ®æ¦‚è§ˆ":
        display_data_overview()
    elif selected_menu == "ğŸ” è´¨é‡è¯„ä¼°":
        display_data_quality()
    elif selected_menu == "ğŸª å¸‚åœºåˆ†æ":
        display_market_analysis()
    elif selected_menu == "ğŸ“‹ æ•°æ®æ²»ç†":
        display_data_lineage()
    elif selected_menu == "ğŸ“¤ æ•°æ®å¯¼å‡º":
        export_data_section()
    
    # Footer
    st.sidebar.markdown("---")
    st.sidebar.markdown("**Menarini Asia Pacific**")
    st.sidebar.markdown("Data & AI Engineering Team")
    st.sidebar.markdown(f"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M')}")

if __name__ == "__main__":
    main()
